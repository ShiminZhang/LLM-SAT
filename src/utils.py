import torch    
import gc
import time
import psutil
import logging
import wandb

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

def log_memory_usage(stage_name, wandb_enabled=False):
    """è®°å½•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    gpu_info = get_gpu_memory_info()
    cpu_memory = psutil.virtual_memory()
    
    memory_info = {
        f"gpu_total_gb": gpu_info["total"] if gpu_info else 0,
        f"gpu_allocated_gb": gpu_info["allocated"] if gpu_info else 0,
        f"gpu_reserved_gb": gpu_info["reserved"] if gpu_info else 0,
        f"gpu_free_gb": gpu_info["free"] if gpu_info else 0,
        f"gpu_utilization_pct": gpu_info["utilization"] if gpu_info else 0,
        f"cpu_memory_used_pct": cpu_memory.percent,
        f"cpu_memory_available_gb": cpu_memory.available / 1024**3,
        "stage": stage_name
    }
    
    logger.info(f"ğŸ“Š {stage_name} - GPU: {gpu_info['utilization']:.1f}% used, {gpu_info['free']:.1f}GB free" if gpu_info else f"ğŸ“Š {stage_name} - CPU: {cpu_memory.percent:.1f}% used")
    
    if wandb_enabled:
        wandb.log(memory_info)
    
    return memory_info
def get_gpu_memory_info():
    """è·å–GPUæ˜¾å­˜ä¿¡æ¯"""
    if torch.cuda.is_available():
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3  # GB
        gpu_allocated = torch.cuda.memory_allocated(0) / 1024**3  # GB
        gpu_reserved = torch.cuda.memory_reserved(0) / 1024**3  # GB
        gpu_free = gpu_memory - gpu_reserved
        return {
            "total": gpu_memory,
            "allocated": gpu_allocated,
            "reserved": gpu_reserved,
            "free": gpu_free,
            "utilization": (gpu_reserved / gpu_memory) * 100
        }
    return None


def force_cleanup_gpu():
    """å¼ºåˆ¶æ¸…ç†GPUæ˜¾å­˜"""
    if torch.cuda.is_available():
        # æ¸…ç†PyTorchç¼“å­˜
        torch.cuda.empty_cache()
        torch.cuda.ipc_collect()
        
        # æ¸…ç†Pythonåƒåœ¾å›æ”¶
        gc.collect()
        
        # å¼ºåˆ¶åŒæ­¥ï¼Œç¡®ä¿æ¸…ç†å®Œæˆ
        torch.cuda.synchronize()
        
        # å†æ¬¡æ¸…ç†
        torch.cuda.empty_cache()
        gc.collect()
        
        # ç­‰å¾…ä¸€å°æ®µæ—¶é—´è®©ç³»ç»Ÿå¤„ç†
        time.sleep(1)